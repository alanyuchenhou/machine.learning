\documentclass[12pt]{article}
\renewcommand*{\familydefault}{\sfdefault}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{fullpage}
\usepackage{tabularx}
\usepackage{graphicx}
\begin{document}
\title{CS570 Machine Learning Homework 1}
\author{Yuchen Hou}
\maketitle

\section{Decision boundary of voted and average perceptron}
\subsection{Voted perceptron}
No, it is not linear. Its classifier is
\begin{align}
  f_{voted}(x) = sign (\sum_{i=1}^m c_i sign(w_i \cdot x))
\end{align}
which can not be written in the form of
\begin{align}
  f_{voted}(x)= sign (w_{voted} \cdot x)
\end{align}
\subsection{Average perceptron}
Yes, it is linear. Its classifier is
\begin{align}
  f_{average}(x)
  &= sign (\sum_{i=1}^m (c_i w_i \cdot x))\\
  &= sign ((\sum_{i=1}^m c_i w_i) \cdot x)\\
  &= sign (w_{average} \cdot x)
\end{align}
where
\begin{align}
  w_{average} = (\sum_{i=1}^m c_i w_i)
\end{align}
This is obviousely a linear classifier with weight vector $w_{average}$. It has a linear decision boundary
\begin{align}
  w_{average} \cdot x = 0
\end{align}

\section{Passive-Agressive weight update margin}
The linear weight update is
\begin{align}
  w_{t+_1} = w_{t} + \tau y_{t} x_{t}
\end{align}
The margin constraint
\begin{align}
  M
  &= y_{t} (w_{t+1} \cdot x_{t})\\
  &= y_{t} ((w_{t} + \tau y_{t} x_{t}) \cdot x_{t})\\
  &= y_{t} (w_{t} \cdot x_{t} + \tau y_{t} x_{t} \cdot x_{t})\\
  &= y_{t} w_{t} \cdot x_{t} + \tau x_{t} \cdot x_{t}
\end{align}
\begin{align}
  \tau = \frac{M - y_{t} w_{t} \cdot x_{t}}{ x_{t} \cdot x_{t}}
\end{align}
When margin is greater than M, there is no update. In conclusion
\begin{equation}
  \tau =
  \begin{cases}
      0 & y_{t} (w_{t+1} \cdot x_{t}) \geq M \\
      \frac{M - y_{t} w_{t} \cdot x_{t}}{ x_{t} \cdot x_{t}} & y_{t} (w_{t+1} \cdot x_{t}) < M
  \end{cases}
\end{equation}

\section{Weight update algorithm with margin error}
The desired algorithm can be viewed as a more general form of PA algorithm. The PA algorithm updates the weight vector to achieve margin 1 everytime the margin drops below 1. The desired algorithm for this problem should update the weight vector to achieve margin $\gamma$ everytime the margin drops below $\gamma$. A margin error refers to a unconfident but correct classification where margin is between 0 and $\gamma$. The goal of ensuring all the points in $B(x_{t},\gamma)$ having the same label (which should be a correct lable, as the classification of $x_{t}$ is correct), can be realized by achieving a margin of $\gamma$ at the center of the point $x_{t}$. The margin constraint for this algorithm should be
\begin{align}
  \gamma = y_{t} (w_{t+1} \cdot x_{t})
\end{align}
Following the same approach in previous problem (substituting $M$ with $\gamma $), the linear weight update is
\begin{align}
  w_{t+_1} = w_{t} + \tau y_{t} x_{t}
\end{align}
where
\begin{equation}
  \tau =
  \begin{cases}
      0 & y_{t} (w_{t+1} \cdot x_{t}) \geq \gamma  \\
      \frac{\gamma  - y_{t} w_{t} \cdot x_{t}}{ x_{t} \cdot x_{t}} & y_{t} (w_{t+1} \cdot x_{t}) < \gamma 
  \end{cases}
\end{equation}

\section{Empirical analysis}
\subsection{The training for Perceptron and PA algorithm}
\begin{figure}[htb]
  \centering
      {\includegraphics[width=1\linewidth]{result.png}} \rule{1\linewidth}{1pt}
      \caption{The mistake counts during training for Perceptron and PA algorithm.}
      \label{fig:result}
\end{figure}
The two learning curves are shown in Figure \ref{fig:result}. My observations are listed below.
\begin{enumerate}
  \item They both learn very fast during the first 10 iterations of their training.
  \item They both improve gradually during the following iterations with small fluctuations in their progress.
  \item Perceptron's learning progress is very similar to that of PA algorithm.
\end{enumerate}
\subsection{The accuracy of Perceptron and PA algorithm}
\begin{figure}[htb]
  \centering
      {\includegraphics[width=1\linewidth]{accuracy.png}} \rule{1\linewidth}{1pt}
      \caption{The accuracy during training and validation for Perceptron and PA algorithm.}
      \label{fig:accuracy}
\end{figure}
The 4 accuracy curves are shown in Figure \ref{fig:accuracy}. My observations are listed below.
\begin{enumerate}
  \item All 4 accuracy improve very fast during the first 10 iterations of training.
  \item They both improve gradually during the following iterations with small fluctuations in their progress.
  \item Perceptron's accuracy is very similar to that of PA algorithm.
  \item The accuracy on validation data is higher than that on training data; and it achieves 1 in about 50 iterations.
\end{enumerate}
\subsection{The comparison of the accuracy of Perceptron and PA algorithm}
The accuracies of both algorithm achieve 1 so each one is as good as each other.
\subsection{Shuffling the training data}
Presumably shuffling improves learning progress and can speedup the increase of accuracy. However, I didn't mangage to observe any significant effect of shuffling.
\subsection{Variable learning rate}
\begin{figure}[htb]
  \centering
      {\includegraphics[width=1\linewidth]{accuracyVariable.png}} \rule{1\linewidth}{1pt}
      \caption{The accuracy during training and validation for Perceptron with fixed and variable learning rate.}
      \label{fig:accuracyVariable}
\end{figure}
The 4 accuracy curves are shown in Figure \ref{fig:accracyVariable}. I didn't observe any effect of variable learning rate. Decreasing learning reduces the effect of latter samples on the weight vector. I am not sure if this stratergy nessessarily improves the learning.
\subsection{Variable training examples}
\begin{figure}[htb]
  \centering
      {\includegraphics[width=1\linewidth]{variableTraining.png}} \rule{1\linewidth}{1pt}
      \caption{The learning curve of perceptron and PA algorithm for different number of training samples.}
      \label{fig:variableTraining}
\end{figure}
The 2 accuracy curves are shown in Figure \ref{fig:variableTraining}. Both of the 2 accuracy curves are very close to 1 regardless of training iterations. My explanation is even 1000 training examples are more than enough for the learning algorithm to accurately classify samples.
\end{document}
