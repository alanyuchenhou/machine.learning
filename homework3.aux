\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Nearest neighbors}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Distance calcuation}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Computation efficiency}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Locality Sensitive Hashing}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Problem}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Approach}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Locality-sensitive hashing algorithm}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Rule set and decision tree conversion}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Convertable R}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Inconvertable R}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {4}ID3 decision tree learning algorithm}{3}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces conditional entropy and information gain for every splitting rule z}}{3}}
\newlabel{tab:entropy}{{1}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Empirical analysis}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}K-nearest neighbor classifier}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Decision tree classifier (unpruned)}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Decision tree classifier (pruned)}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The k-nearest neighbor classifier accuracy for different k parameter for training, validation and testing data with weka.classifiers.lazy.IBk}}{6}}
\newlabel{fig:accuracy}{{1}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The decision tree classifier accuracy for different c parameter for training, validation and testing data with weka.classifiers.trees.J48}}{7}}
\newlabel{fig:accuracy_decision_tree}{{2}{7}}
