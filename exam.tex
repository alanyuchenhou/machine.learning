\documentclass[12pt]{article}
\renewcommand*{\familydefault}{\sfdefault}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{fullpage}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{multicol}

\begin{document}
\begin{multicols}{2}
  \section{classification}
\end{multicols}{}
\begin{table}[htb]
  \centering
  \begin{tabularx}{\textwidth}{|l|l|X|X|l|} \hline
    classifier name & classifier type & object to learn & parameter count & prior belief \\ \hline
    naive bayes & probabilistic::generative & $p(y), p(x|y)$ & fixed, parametric & bayesian  \\ \hline
    logistic regression & probabilistic::discriminative & $p(y|x)$ & unfixed, nonparametric & nonbayesian \\ \hline
    decision tree & deterministic & decision tree & unfixed, nonparametric & nonbayesian \\ \hline
  \end{tabularx}
  \caption{different classifiers}
\end{table}
\begin{multicols}{2}

  \section{2-class linear classifier}
  \begin{description}
  \item[label] $y = 1, -1$
  \item[weight vector] $w$
  \item[linear prediction] $f(x) = w \cdot x + b$
  \item[classification] $\hat{y} = sign(f(x))$
  \item[functional margin] $M = y \cdot f(x)$
  \item[geometric margin] $M = \frac{y \cdot f(x)}{||w||}$
  \item[natural (0-1) loss]
    $l_{01} =
    \begin{cases}
      0 & M > 0 \\
      1 & M < 0
    \end{cases}$
  \item[hinge loss]
    $l_{h} =
    \begin{cases}
      0 & M > 1 \\
      1 - M & M < 1
    \end{cases}$
    \item[linearly separable] $\forall i, M(x_i, y_i) > 0$
  \end{description}

  \section{multi-class linear classifier}
  \begin{description}
  \item[label] $y = 1, 2...k$
  \item[weight vector] $w = (w_1, w_2...w_k)$
  \item[feature vector] $F(x,4) = (0,0,0,x,0...0)$
  \item[update vector] $\delta x_t = F(x_t,y_t) - F(x_t,\hat{y_t})$
  \item[linear prediction] $f_r(x) = w \cdot F(x,r) + b$
  \item[classification] $\hat{y} = argmax_r(f_r(x))$
  \item[margin] $M = f_{y_t}(x) - f_{\hat{y_t}}(x)$
  \item[hinge loss]
    $l_{h} =
    \begin{cases}
      0 & M > 1 \\
      1 - M & M < 1
    \end{cases}$
  \end{description}

  \section{online learning}
  \begin{description}
  \item[learning method] online update
  \item[examples] perceptron, passive-agressive
  \item[2-class weight update]
    for t = 1, 2...n \\
    $w_{t+_1} = w_{t} + \tau y_{t} x_{t}$
  \item[multi-class weight update]
    for t = 1, 2...n \\
    $w_{t+_1} = w_{t} + \tau \delta x_t$
  \item[tradeoff] corrective, conservative
  \end{description}

  \section{perceptron}
  \begin{description}
    \item[update rate] $\tau = l_{01}$
    \item[weights and lifespan]  $(w_1, c_1), (w_1, c_1) ... (w_n, c_n)$
    \item[linear prediction] $f_s(x) = w_s \cdot x + b$
    \item[voted] $f_{voted}(x) = \sum_{i=1}^n c_i sign(f_s(x))$
    \item[average] $f_{average}(x) = \sum_{i=1}^n c_i f_s(x)$
  \end{description}

  \section{passive-agressive}
  \begin{description}
    \item[desired margin] $m$
    \item[update goal] $M_t \geq m$
    \item[2-class update rate]
      $\tau =
      \begin{cases}
        0 & M_t \geq m \\
        \frac{m - M_t}{x_{t}^2} & M_t < m
      \end{cases}$
    \item[2-class $\tau$ calculation]
      \begin{align*}
        m
        &= y_{t} ((w_{t} + \tau y_{t} x_{t}) \cdot x_{t})\\
        &= y_{t} (w_{t} \cdot x_{t} + \tau y_{t} x_{t} \cdot x_{t})\\
        &= y_{t} w_{t} \cdot x_{t} + \tau x_{t} \cdot x_{t} \\
        \tau
        &= \frac{m - y_{t} w_{t} \cdot x_{t}}{ x_{t} \cdot x_{t}}
      \end{align*}
    \item[multi-class update rate]
      $\tau =
      \begin{cases}
        0 & M_t \geq m \\
        \frac{m - M_t}{\delta x_t^2} & M_t < m
      \end{cases}$
  \end{description}

  \section{batch learning}
  \begin{description}
  \item[learning method] global optimization
  \item[example] CLOSE, SVM
  \item[weight vector construction] $w = \sum_{i=1}^n \alpha_i y_i x_i$
  \item[support vector] $x_i$ when $ \alpha_i \neq 0$
  \end{description}

  \section{CLOSE}
  \begin{description}
  \item[positive center] $C_+ = \frac{1}{n_+}\sum_{i:y_i=+1}x_i$
  \item[negative center] $C_- = \frac{1}{n_-}\sum_{i:y_i=-1}x_i$
  \item[prediction rule] $minarg_r (||x-C_r||)$
  \item[CLOSE prediction]
    \begin{align*}
      f(x)
      &= (x - C_-)^2 - (x - C_+)^2 \\
      &= -2x \cdot C_- + C_-^2 - (-2x \cdot C_+ + C_+^2) \\
      &=  2 (C_+ - C_-) x + C_-^2 - C_+^2 \\
      &=  w \cdot x + b \\
      w &=  2 (C_+ - C_-) \\
      b &=  C_-^2 - C_+^2 
    \end{align*}
  \item[weight vector]
    \begin{align*}
      w &=  2 (C_+ - C_-) \\
      &= 2 (\frac{1}{n_+}\sum_{i:y_i=+1}x_i - \frac{1}{n_-}\sum_{i:y_i=-1}x_i) \\
      &= \sum_{i:y_i=+1} \frac{2}{n_+} (+1) x_i + \sum_{i:y_i=-1} \frac{2}{n_-} (-1) x_i \\
      &= \sum_{i=1}^{n_+ + n_-} \alpha_i y_i x_i
    \end{align*}
    where
    \begin{align*}
      \alpha_i =
      \begin{cases}
        \frac{2}{n_+} & i:y_i=+1 \\
        \frac{2}{n_-} & i:y_i=-1
      \end{cases}
    \end{align*}
  \item[support vector] $\forall i, x_i$ is a support vector 
  \end{description}

  \section{nonlinear classifier}
  \begin{description}
    \item[ld feature] low D $x_{ld}$ (not linearly sepearble)
    \item[hd feature] high D $x_{hd}$ (linearly sepearble)
    \item[feature space mapping] $x_{hd} = \phi(x_{ld})$
    \item[hd inner product] $x_{hd1} \cdot x_{hd2}$
    \item[ld kernel function] $K(x_{ld1}, x_{ld2}) = x_{hd1} \cdot x_{hd2}$
    \item[nonlinear prediction] $f(x) = \sum_{i} \alpha_i y_i K(x_i, x) + b$
    \item[kernel matrix]
      $k =
      \begin{pmatrix}
        K(x_1, x_1) & \cdots & K(x_1, x_m) \\
        \vdots & \ddots & \vdots \\
        K(x_m, x_1) & \cdots & K(x_m, x_m)
      \end{pmatrix}$
    \item[Mercer's theorem] K is kernel $\iff$ k is symmetric and positive semidefinite
      \item[symmetric] $\forall x_i, x_j; K(x_i,x_j) = K(x_j,x_i)$
      \item[positive] $\forall t; t^TKt \geq 0$
  \end{description}

  \section{High dimensional feature space distance}
  \begin{align*}
    ||\phi(x_i)-\phi(x_i)||^2
    =& \phi(x_i) \phi(x_i) - 2\phi(x_i) \phi(x_j) + \phi(x_j) \phi(x_j) \\
    =& K(x_i, x_i) -2 K(x_i, x_j) + K(x_j, x_j) \\
    =& 1 -2 exp(-\frac{1}{2} (x_i - x_j)^2) + 1
  \end{align*}
  As
  \begin{align*}
    -2 exp(-\frac{1}{2} (x_i - x_j)^2) < 0
  \end{align*}
  we have
  \begin{align*}
    ||\phi(x_i)-\phi(x_i)||^2 < 2
  \end{align*}
  \section{Remote testing vector}
  \begin{align*}
    f(x_f; \alpha,b)
    =& \sum_{i} \alpha_i y_i K(x_i, x_f) + b \\
    =& \sum_{i} \alpha_i y_i exp(-\frac{1}{2} (x_i - x_f)^2) + b
  \end{align*}
  Therefore
  \begin{align*}
    & \lim_{x_i-x_f \to \infty} f(x_f; \alpha,b) \\
    =& \sum_{i} \alpha_i y_i \lim_{x_i-x_f \to \infty} exp(-\frac{1}{2} (x_i - x_f)^2) + b \\
    =& \sum_{i} \alpha_i y_i \cdot 0 + b \\
    =& b
  \end{align*}
  \section{Kernel property}
  No. This kernel violates the positivity in the following case
  \begin{align*}
    m &= 1 \\
    x_m &= 1 \\
    t &= 1
  \end{align*}
  Because in this case
  \begin{align*}
    K =& (-1) \\
    & t^T K t \\
    =& 1 \cdot (-1) \cdot 1 \\
    =& -1 \\
    <& 0
  \end{align*}
  where the positivity asserts
  \begin{align*}
    t^T K t > 0
  \end{align*}

  \section{Nearest neighbors}
  \begin{description}
    \item[prediction goal] predict the test vector to closest training vector
    \item[prediction rule]
      \begin{align*}
        \hat{y} &= y_i\\
        i &= argmin_i (||x_i - x||)
      \end{align*}
  \end{description}

  \subsection{Distance calcuation}
  First, we can reduce the inequality to be prooved
  \begin{align*}
    & ( \frac{1}{\sqrt{d}} \sum_{i=1}^{d}x_i - \frac{1}{\sqrt{d}} \sum_{i=1}^{d}z_i )^2 \leq \sum_{i=1}^{d}(x_i - z_i)^2 \\
    \iff & ( \frac{1}{d} ( \sum_{i=1}^{d}x_i - \sum_{i=1}^{d}z_i) )^2 \leq \frac{1}{d} \sum_{i=1}^{d} (x_i - z_i)^2 \\
    \iff & ( \frac{1}{d} \sum_{i=1}^{d}(x_i-z_i) )^2 \leq \frac{1}{d} \sum_{i=1}^{d} (x_i - z_i)^2 \\
    \iff & E(y)^2 \leq E(y^2)
  \end{align*}
  where $y_i=x_i-z_i$. This can be prooved using Jensen's inequality with convex function $ f(y) = y^2 $
  \begin{align*}
    f(E(y)) &\leq E(f(y)) \\
    E(y)^2 &\leq E(y^2)
  \end{align*}
  \subsection{Computation efficiency}
  Using the above property, we can evaluate the distance as
  \begin{align*}
    ( \sum_{i=1}^{d}(x_i-z_i) )^2
  \end{align*}
  which is no more than the Euclidean distance
  \begin{align*}
    \sum_{i=1}^{d} (x_i - z_i)^2
  \end{align*}
  In this way, we can make the computation efficient because now the computation reduces multiplication from d to 1, and it does not change the numbers of other computation operations.
  \section{Locality Sensitive Hashing}
  In this paper, Andoni and Indyk give an overview of several approximate and exact nearest neighbor problem.
  \subsection{Problem}
  The problem considered in this paper is slightly different than the problem we discussed in class. The nearest neighbor problem we considered in class required the search for the nearest neighbor, while the the problem in this paper essentially requires only a near neighbor. The rationale is a near neighbor is good enough and the nearest one is not necessary in many situations.
  \subsection{Approach}
  The key of searching for near neighbor efficiently is a suitable data structure to store the data. In this particular case, several hash funtions hash the objects into buckets so that 2 objects closer in distance are more likely to be hashed to same bucket. 
  \subsection{Locality-sensitive hashing algorithm}
  Locality-sensitive hashing is realized by a set of L hashing functions G = {$g_1(), g_2(),...g_L()$}, where each hashing function $g_i()$ maps a point p into a bucket $g_i(p)$ (equivalently, $g_i()$ maps the point space P into a hash table), and $g_i()$ is a concatenation of several member functions from a locality sensitive hashing function family. To search for query point q, a near neighbor can be selected from a bucket that q is hashed to. By scanning through all the buckets $g_1(q)...g_L(q)$, all near neighbors of q can be found. There are multiple flavors of the algorithm regarding their ways of finish. It can stop searching as soon as a number of neighbbors are found, or after a complete search through all the hash tables. It can report all the objects that it finds, or only a number of the nearest ones.
  \section{decision tree}
  The possibility of the conversion depends on the rule set R. I will show 1 simple example for each case: one for a convertable R and one for a inconvertable R. Through these 2 examples we can conclude that only Rs with some restrictions can be converted to a decision tree.
  \subsection{Convertable R}
  R = \{r0: if x $<$ 3 then class = 0; r1: if x $\geq$ 3 then class = 1;\}. In this case, R can be converted to a decision tree with only one non-leaf node of condition x $<$ 3.
  \subsection{Inconvertable R}
  R = \{r0: if x $<$ 0 then class = 0; r1: if x $>$ 3 then class = 1;\}. In this case, R cannot be conveted to a decision tree as there are no rules for the range of [0, 3].
  \section{ID3 decision tree training}
  \begin{description}
    \item[entropy] $H(Y) = - \sum_{y} (P(Y = y) lg(P(Y = y)))$
    \item[] $H(Y|Z=z) = - \sum_{y} (P(Y = y|Z=z) lg(P(Y = y|Z=z)))$
    \item[conditional entropy] $H(Y|Z) = \sum_{z} (P(Z = z) H(Y | Z = z))$
    \item[information gain] $G(Z) = H(Y) - H(Y|Z)$
    \item[training goal] maximize $G(Z)$ at every node
    \item[decision variable] Z (e.g. $1(x_2 < 5)$)
  \end{description}
\end{multicols}{}
  \begin{table}[htb]
    \centering
    \begin{tabularx}{\textwidth}{|l|l|l|l|l|l|l|l|X|} \hline
      z & P(!y$|$!z) & P(y$|$!z) & P(!z) & P(!y$|$z) & P(y$|$z) & P(z) & H(Y$|$Z) & G(Z) \\ \hline
      $outlook<sunny$ & 2/9 & 7/9 & 9/14 & 3/5 & 2/5 & 5/14 & 0.8380 & 0.1022 \\ \hline
      %% outlook==overcast & 5/10 & 5/10 & 10/14 & 0/4 & 4/4 & 4/14 & 0.7142 & 0.2260 \\ \hline
      $outlook<overcast$ & 3/9 & 6/9 & 9/14 & 2/5 & 3/5 & 5/14 & 0.9371 & 0.0031 \\ \hline
      $temp<hot$ & 2/4 & 2/4 & 4/14 & 3/10 & 7/10 & 10/14 & 0.9152 & 0.0250 \\ \hline
      %% temp==mild & 3/8 & 5/8 & 8/14 & 2/6 & 4/6 & 6/14 & 0.9389 & 0.0013 \\ \hline
      $temp<mild$ & 4/10 & 6/10 & 10/14 & 1/4 & 3/4 & 4/14 & 0.9253 & 0.0149 \\ \hline
      $humidity<high$ & 1/7 & 6/7 & 7/14 & 4/7 & 3/7 & 7/14 & 0.7884 & 0.1518 \\ \hline
      $wind<weak$ & 3/6 & 3/6 & 6/14 & 2/8 & 6/8 & 8/14 & 0.8921 & 0.0481 \\ \hline
    \end{tabularx}
    \caption{conditional entropy and information gain for every splitting rule z}
    \label{tab:entropy}
  \end{table}
\begin{multicols}{2}
  The possible splitting rules and their corresponding conditional entropies are listed below in Table \ref{tab:entropy}, where y stands for playtennis == yes. As the features' values are not numerical but enumerated, I have used the following assumptions for the enumerated values to enable decision tree classifier: $sunny > overcast > rain, hot > mild > cool$. The maximum information gain is achieved when the the feature humidity is selected to make the decision.

  \section{probabilistic classifier}
  \begin{description}
    \item[maximum posterior probability classification] $\hat{y} = argmax_k(p(Y=k|X=x))$
    \item[minimum expected loss classification] $\hat{y} = argmin_k \sum_j l(\hat{y}=k,y=j)(p(Y=j|X=x))$
      \item[example] logistic regression, naive bayes
  \end{description}

  \section{Probability classifier asymptotic accuracy}
  Research by Ng and Jordan has a comparison between the 2 probability classifiers: logistic regression has lower asymptotic error; naive Bayes approaches its asymptotic error faster \cite{jordan2002discriminative}. I have not fully understood their specific theoretical argument and their experiments on 15 datasets only partly support their claim: 8 out of 15 of their experiments support their claim, while the rest 7 out of 15 experiments do not directly support, but are believed to support their claim if the experiments include sufficient large datasets. I guess the reason lies in a general rule in many machine learning methods: simpler and more direct approaches have better scalability (in this case, logistic regression's approach).
  \subsection{}
  Logistic regression is better than or the same as naive Bayes, because logistic regression is simpler and more direct than naive Bayes.
  \subsection{}
  Logistic regression is better than naive Bayes. Beyond the reason mentioned above, another reason is that the conditional independence of the features given the lable does not hold any more, which is critical for naive Bayes to be accurate.

  \section{naive bayes}
  \begin{description}
  \item[assumption]
    \begin{align*}
      X &= (X_1, X_2, ... X_d) \\
      x_i &\in \{a_1, a_2, ... a_m\} \\
      y &\in \{b_1, b_2, ... b_n\}
    \end{align*}
  \item[parameters to learn] $p(Y), p(X_i|Y)$
  \item[bayes rule] $p(y|x) = \frac{p(x,y)}{p(x)}$
  \item[] $p(x) = \sum_y p(x,y)$
  \item[] $p(x,y) = p(y) \cdot p(x|y)$
  \item[naive bayes assumption]
    \begin{align*}
      p(X=x|Y=y)
      &= p(X_1=x_1, ... X_d=x_d|Y=y) \\
      &= \prod_{i=1}^d p(X_i=x_i|Y=y)
    \end{align*}
  \item[laplace smoothing] $p(z=k) = \frac{1+n_k}{2+\sum_i n_i}$
  \end{description}
  
  \section{2-class logistic regression}
  \begin{description}
  \item[linear prediction] $f(x) = w \cdot x$
  \item[posterior probability] $p(y=1|x)=\frac{1}{1+exp(-f(x))}$
  \item[linearality] $p(y=1|x) > \theta \iff f(x) > \eta$
  \item[learning bojective function, log likelihood function] $l(w) = \sum_i log (p(Y=y^i|X=x^i))$
  \item[training goal] maximize learning objective
  \item[optimum weight vector] $w = argmax_w l(w)$
  \item[online learning weight update] while(not converge)\\
    $w_{t+1} = w_t +  \tau \cdot x_t$
  \item[update rate] $\tau = \sum_(t y_t-p(y=1|x_t))$
  \end{description}

  No. The only parameter logistic regression learns is the weight vector like a linear classifier, it does not estimate any feature, lable, conditional or joint distribution. Therefore, there are not enough parameters to calculate the feature distribution.

  \section{multiclass logistic regression}
  \begin{description}
  \item[posterior probability] $p(Y=k|X=x) = \frac{exp(w_k \cdot x)} { \sum_j exp(w_j \cdot x)}$
  \end{description}
  \subsection{gradient ascent}
  Using gradient ascent
  %% p(Y=y^i|X=x^i) = \frac{exp(w_{y^i} \cdot {x^i})} { \sum_j exp(w_j \cdot {x^i})}
  \begin{align*}
    v
    &= w_k \\
    y_k^i &=
    \begin{cases}
      1 & y^i = k \\
      0 & y^i \neq k
    \end{cases} \\
    \frac {\partial l_i} {\partial v_m}
    &= \frac {\partial log (p(Y=y^i|X=x^i))}{\partial v_m} \\
    &= \frac{1}{(p(Y=y^i|X=x^i))} \frac {\partial (p(Y=y^i|X=x^i))}{\partial v_m} \\
    &= \frac{1}{(\frac{exp(w_{y^i} \cdot {x^i})} { \sum_j exp(w_j \cdot {x^i})})} \frac {\partial (\frac{exp(w_{y^i} \cdot {x^i})} { \sum_j exp(w_j \cdot {x^i})})}{\partial v_m} \\
    &= (y_k^i - \frac{exp(w_k \cdot x^i)} { \sum_j exp(w_j \cdot x^i)}) x_m^i \\
    &= (y_k^i - p(Y=k|X=x_i)) x_m^i \\
    \frac {\partial l} {\partial v_m}
    &= \sum_i \frac {\partial l_i} {\partial v_m} \\
    &= \sum_i (y_k^i - p(Y=k|X=x_i)) x_m^i \\
    \nabla_{w_k} l
    &= \sum_m \frac{\partial l}{\partial {v_m}} \\
    &= \sum_i (y_k^i - p(Y=k|X=x^i))x^i
  \end{align*}
  \subsection{regularization term}
  The original update rule is
  \begin{align*}
    \nabla_{w_k} l  &= \sum_i (y_k^i - p(Y=k|X=x^i))x^i \\
    w &= w + \eta \nabla_{w_k} l
  \end{align*}
  After adding a regularization term to the objective function
  \begin{align*}
    l'(w)
    &= l(w) - \lambda \sum_k w_k^2 \\
    \nabla_{w_k} l'
    &= \nabla_{w_k} (l(w) - \lambda \sum_k w_k^2) \\
    &= \sum_i ((y_k^i - p(Y=k|X=x^i))x^i) - 2 \lambda w_k \\
    w
    &= w + \eta \nabla_{w_k} l'
  \end{align*}

\end{multicols}{}
\end{document}
