\documentclass[12pt]{article}
\renewcommand*{\familydefault}{\sfdefault}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{fullpage}
\usepackage{tabularx}
\usepackage{graphicx}
\begin{document}
\title{CS570 Machine Learning Homework 2}
\author{Yuchen Hou}
\maketitle

\section{CLOSE classifier}
According to the definition of CLOSE classifier, the following prediction rule satisfies the requirement
\begin{align}
  f(x) = (x - C_-)^2 - (x - C_+)^2
\end{align}
which can be reduced as
\begin{align}
  f(x) &= -2x \cdot C_- + C_-^2 - (-2x \cdot C_+ + C_+^2) \\
  &=  2 (C_+ - C_-) x + C_-^2 - C_+^2 \\
  &=  w \cdot x + b
\end{align}
where
\begin{align}
  w &=  2 (C_+ - C_-) \\
  b &=  C_-^2 - C_+^2 
\end{align}
and the decision rule should be
\begin{align}
  \hat{y} &= sign(w \cdot x + b)
\end{align}
The weight vector can be written as a linear combination of all training examples
\begin{align}
  w &=  2 (C_+ - C_-) \\
  &= 2 (\frac{1}{n_+}\sum_{i:y_i=+1}x_i - \frac{1}{n_-}\sum_{i:y_i=-1}x_i) \\
  &= \sum_{i:y_i=+1} \frac{2}{n_+} (+1) x_i + \sum_{i:y_i=-1} \frac{2}{n_-} (-1) x_i \\
  &= \sum_{i=1}^{n_+ + n_-} \alpha_i y_i x_i
\end{align}
where
\begin{align}
  \alpha_i =
  \begin{cases}
    \frac{2}{n_+} & i:y_i=+1 \\
    \frac{2}{n_-} & i:y_i=-1
  \end{cases}
\end{align}
A training vector is a support vector if and only if its dual weight is not zero. In this problem, all the dual weights are not zero so all the $(n_+ + n_-)$ training vectors are support vectors.
\section{Higher dimensional feature space distance}
\begin{align}
  & ||\phi(x_i)-\phi(x_i)||^2 \\
  =& \phi(x_i) \phi(x_i) - 2\phi(x_i) \phi(x_j) + \phi(x_j) \phi(x_j) \\
  =& K(x_i, x_i) -2 K(x_i, x_j) + K(x_j, x_j) \\
  =& 1 -2 exp(-\frac{1}{2} (x_i - x_j)^2) + 1
\end{align}
As
\begin{align}
  -2 exp(-\frac{1}{2} (x_i - x_j)^2) < 0
\end{align}
we have
\begin{align}
  ||\phi(x_i)-\phi(x_i)||^2 < 2
\end{align}
\section{Remote testing vector}
\begin{align}
  & f(x_f; \alpha,b) \\
  =& \sum_{i} \alpha_i y_i K(x_i, x_f) + b \\
  =& \sum_{i} \alpha_i y_i exp(-\frac{1}{2} (x_i - x_f)^2) + b
\end{align}
Therefore
\begin{align}
  & \lim_{x_i-x_f \to \infty} f(x_f; \alpha,b) \\
  =& \sum_{i} \alpha_i y_i \lim_{x_i-x_f \to \infty} exp(-\frac{1}{2} (x_i - x_f)^2) + b \\
  =& \sum_{i} \alpha_i y_i \cdot 0 + b \\
  =& b
\end{align}
\section{Kernel property}
No. This kernel violates the positivity in the following case
\begin{align}
  m &= 1 \\
  x_m &= 1 \\
  t &= 1
\end{align}
Because in this case
\begin{align}
  K =& (-1) \\
  & t^T K t \\
  =& 1 \cdot (-1) \cdot 1 \\
  =& -1 \\
  <& 0
\end{align}
where the positivity asserts
\begin{align}
   t^T K t > 0
\end{align}
\section{Empirical analysis}
\begin{figure}[htb]
  \centering
      {\includegraphics[width=1\linewidth]{homework2_1.png}} \rule{1\linewidth}{1pt}
      \caption{The accuracy vs C parameter for training, validation and testing with linear kernel}
      \label{fig:accuracy1}
\end{figure}
With a linear kernel, the training, validation and testing accuracies varies significantly with the value of C parameter. Except for very small C, training accuracy is alwasys the highest, while validation and testing accuracy are very close. C is used to control the functional margin: the large C is, the smaller the margin is. In Figure \ref{fig:accuracy1}, we can see for very large C, as the margin is small, the fitting of the training examples (training accuracy) is very high; the smaller C is, the larger the margin is, and fitting training example becmoes more difficult, and training accuracy becomes much lower. On the other hand, for validation and testing data, the situation is similar, except for  after some point with C value around  (0.01, 0.1), the accuracy doesn't improve anymore, and actually decreases a little bit, due to large margin and higher generaliztion error.
\begin{figure}[htb]
  \centering
      {\includegraphics[width=1\linewidth]{homework2_2.png}} \rule{1\linewidth}{1pt}
      \caption{The accuracy vs C parameter for training, validation and testing with polynomial kernel of degree 2}
      \label{fig:accuracy2}
\end{figure}
As shown in Figure \ref{fig:accuracy2}, After replacing the linear kernel with a polynomial kernel of degree 2, the training accuracy and validation accuracy do not manifest significant changes from their linear kernel counterparts. However, the testing accuracy differs from its linear kernel counterpart in two ways: firstly, when C drops below 0.1, the accuracy is very low, which means this linear kernel is more sensitive to noise; secondly, when C exceeds 100, the accuracy can still improve, which shows the kernel's good performance in high slack situations.
\begin{figure}[htb]
  \centering
      {\includegraphics[width=1\linewidth]{homework2_3.png}} \rule{1\linewidth}{1pt}
      \caption{The accuracy vs C parameter for training, validation and testing with polynomial kernel of degree 3}
      \label{fig:accuracy3}
\end{figure}
In Firgure \ref{fig:accuracy3}, the only noticible change from polynomial kernel of degree 2 to degree 3, is that the testing accuracy decreases when C is around the range (0.1, 100). The explanation for this phenomenon is that higer degree kenel is more sensitive to noise due to larger margin.
\begin{figure}[htb]
  \centering
      {\includegraphics[width=1\linewidth]{homework2_4.png}} \rule{1\linewidth}{1pt}
      \caption{The accuracy vs C parameter for training, validation and testing with polynomial kernel of degree 4}
      \label{fig:accuracy4}
\end{figure}
The final variation of the kernel is from polynomial kernel of degree 2 to degree 3, which shows the same trend as before. The difference compared to polynomial kernel of degree 3 is quantitative: the testing accuracy decreases when C is around the range (1, 1000). It further supports the explanation that higer degree kenel is more sensitive to noise due to larger margin.
\end{document}
